{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab4525-b8d9-4a0b-b702-1cc59dbf8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install datasets\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d24a95-99db-481f-944a-0c7e282f2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "# Load the spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to load relation schema from a CSV file\n",
    "def load_relation_schema(csv_file):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Convert 'domains' and 'ranges' columns to strings, replace NaN with empty string\n",
    "    df['domains'] = df['domains'].fillna('').astype(str)\n",
    "    df['ranges'] = df['ranges'].fillna('').astype(str)\n",
    "    # Extract relevant columns and convert to list of tuples\n",
    "    relations = [(row['property'], row['propertyLabel'], row['propertyDescription'], row['domains'], row['ranges']) for _, row in df.iterrows()]\n",
    "    return relations\n",
    "\n",
    "# Load the relation schema from a CSV file\n",
    "relation_schema = load_relation_schema('data/relations.csv')\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')\n",
    "\n",
    "# Check and set the PAD token for the tokenizer\n",
    "pad_token = tokenizer.pad_token\n",
    "if pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# Initialize the Hugging Face pipeline with a generative model\n",
    "generator = pipeline('text-generation', model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer=tokenizer, device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n",
    "\n",
    "\n",
    "\n",
    "#Checks weather a class is a subclass of another class. This is required for the range/domain check.\n",
    "@lru_cache(maxsize=None)\n",
    "def query_subclass_of(subclass_id, class_id):\n",
    "    \"\"\"\n",
    "    Check if the given subclass_id is a subclass of class_id using Wikidata SPARQL endpoint.\n",
    "    Uses caching to reduce duplicate queries.\n",
    "    \"\"\"\n",
    "    sparql_query = f\"\"\"\n",
    "    ASK {{\n",
    "        wd:{subclass_id} (wdt:P279)* wd:{class_id} .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = query_wikidata_sparql(sparql_query)\n",
    "    return response.get('boolean', False)\n",
    "\n",
    "\n",
    "#Checks type compatibility. This is deactivated at the moment.\n",
    "def is_compatible(entity1_classes, entity2_classes, relation_domain, relation_range):\n",
    "    \"\"\"\n",
    "    Check if entity1's classes are compatible with relation_domain and\n",
    "    entity2's classes with relation_range, or vice versa.\n",
    "    \"\"\"\n",
    "    # Assuming query_subclass_of function is defined to check subclass relationships\n",
    "    # and direct class matches.\n",
    "\n",
    "    domain_classes = set(relation_domain.split(', '))\n",
    "    range_classes = set(relation_range.split(', '))\n",
    "\n",
    "    # Check compatibility for entity1 with domain and entity2 with range\n",
    "    compatibility1 = any(query_subclass_of(cls[0], domain[0]) for cls in entity1_classes for domain in domain_classes) \\\n",
    "                     and any(query_subclass_of(cls[0], rng[0]) for cls in entity2_classes for rng in range_classes)\n",
    "\n",
    "    # Check compatibility for entity2 with domain and entity1 with range\n",
    "    compatibility2 = any(query_subclass_of(cls[0], domain[0]) for cls in entity2_classes for domain in domain_classes) \\\n",
    "                     and any(query_subclass_of(cls[0], rng[0]) for cls in entity1_classes for rng in range_classes)\n",
    "\n",
    "    return compatibility1 or compatibility2\n",
    "\n",
    "\n",
    "# Function to generate prompts for relation extraction\n",
    "def generate_prompts(candidate_sentences, relation_schema, linked_entities_dict):\n",
    "    prompts = []\n",
    "    entity_info = []\n",
    "    # Iterate through each candidate sentence to generate prompts\n",
    "    for sentence in candidate_sentences:\n",
    "        entities_in_sentence = [ent.text for ent in sentence.ents if ent.text in linked_entities_dict]\n",
    "        if len(entities_in_sentence) > 1:\n",
    "            for i in range(len(entities_in_sentence)):\n",
    "                for j in range(i + 1, len(entities_in_sentence)):\n",
    "                    entity1_classes = linked_entities_dict[entities_in_sentence[i]]\n",
    "                    entity2_classes = linked_entities_dict[entities_in_sentence[j]]\n",
    "                    # Check compatibility and generate prompt if compatible\n",
    "                    # Inside the generate_prompts function loop\n",
    "                    for _, label, description, relation_domain, relation_range in relation_schema:\n",
    "                        # Adjusted to pass entity classes directly corresponding to domain and range\n",
    "                        #if is_compatible(entity1_classes, entity2_classes, relation_domain, relation_range):\n",
    "                        # Generate prompt if compatible\n",
    "                        prompt = f\"[INST]Answer with yes or no. Does the sentence '{sentence.text}' contain the relationship '{label}' between '{entities_in_sentence[i]}' and '{entities_in_sentence[j]}'? Only answer with yes, if you are sure[/INST]\"\n",
    "                        prompts.append(prompt)\n",
    "                        entity_info.append((entities_in_sentence[i], entities_in_sentence[j], description))\n",
    "    return prompts, entity_info\n",
    "\n",
    "# Function to get entity mentions from a sentence\n",
    "def get_entity_mentions(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [ent.text for ent in doc.ents]\n",
    "\n",
    "# Function to query Wikidata SPARQL endpoint\n",
    "def query_wikidata_sparql(query):\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {'User-Agent': 'TypeCheck; https://enexa.eu/contact/'}\n",
    "\n",
    "    response = requests.get(url,headers=headers, params={'format': 'json', 'query': query})\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "# Function to search for an entity in Wikidata and get its classes\n",
    "def search_wikidata(entity):\n",
    "    search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "        \"search\": entity\n",
    "    }\n",
    "    search_response = requests.get(search_url, params=search_params)\n",
    "    search_results = search_response.json().get(\"search\", [])\n",
    "    if search_results:\n",
    "        wikidata_id = search_results[0].get(\"id\")  # Get the ID of the first match\n",
    "        # Query SPARQL to get all classes for this entity\n",
    "        sparql_query = f\"\"\"\n",
    "        SELECT ?class ?classLabel WHERE {{\n",
    "            wd:{wikidata_id} wdt:P31 ?class .\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        results = query_wikidata_sparql(sparql_query)\n",
    "        # Extract class IDs and labels from query results\n",
    "        classes = [(result['class']['value'].split('/')[-1], result['classLabel']['value']) for result in results['results']['bindings']]\n",
    "        return wikidata_id, classes\n",
    "    return None, []\n",
    "\n",
    "# Function to link entities in sentences to Wikidata\n",
    "def link_entities_to_wikidata(candidate_sentences):\n",
    "    linked_entities = []\n",
    "    # Iterate through candidate sentences to link entities\n",
    "    for sentence in candidate_sentences:\n",
    "        sentence_text = sentence.text  # Convert the span to a string\n",
    "        entity_mentions = get_entity_mentions(sentence_text)\n",
    "        for entity in entity_mentions:\n",
    "            wikidata_id, classes = search_wikidata(entity)\n",
    "            if wikidata_id:\n",
    "                linked_entities.append((entity, wikidata_id, classes))\n",
    "    return linked_entities\n",
    "\n",
    "# Function to extract relations in batch using the generator\n",
    "def batch_extract_relations(candidate_sentences, relation_schema, generator, linked_entities_dict, batch_size=512):\n",
    "    extracted_relations = []\n",
    "    prompts, entity_info_list = generate_prompts(candidate_sentences, relation_schema, linked_entities_dict)\n",
    "    \n",
    "    prompt_generator = (prompt for prompt in prompts)  # Create generator from prompts list\n",
    "\n",
    "    # Iterate through generator responses and process them\n",
    "    for i, response in enumerate(tqdm(generator(prompt_generator, max_new_tokens=4, batch_size=batch_size), desc=\"Processing\")):\n",
    "        generated_text = response[0]['generated_text']\n",
    "        subject, object, relation_description = entity_info_list[i]\n",
    "        # Check if the generated text indicates a \"yes\" response\n",
    "        if \"yes\" in generated_text.lower():\n",
    "            # Find the relation label corresponding to the description\n",
    "            relation_label = next((label for _, label, description, _, _ in relation_schema if description == relation_description), None)\n",
    "            if relation_label:\n",
    "                extracted_relations.append((subject, relation_label, object))\n",
    "\n",
    "    return extracted_relations\n",
    "\n",
    "\n",
    "# Process input text with spaCy to identify candidate sentences for relation extraction\n",
    "text = \"\"\"\n",
    "Berlin is located in Germany.\n",
    "Barack Obama was born in Honolulu.\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "candidate_sentences = [sent for sent in doc.sents if len(sent.ents) >= 2]\n",
    "print(\"Candidated Sentences are:\")\n",
    "print(candidate_sentences)\n",
    "# Link entities to Wikidata for further processing\n",
    "linked_entities = link_entities_to_wikidata(candidate_sentences)\n",
    "print(linked_entities)\n",
    "linked_entities_dict = {entity: classes for entity, _, classes in linked_entities}\n",
    "\n",
    "# Extract relations for all candidate sentences in a batch\n",
    "extracted_relations = batch_extract_relations(candidate_sentences, relation_schema, generator, linked_entities_dict, batch_size=512)\n",
    "\n",
    "# Print extracted relations\n",
    "print(\"Extracted Relations:\")\n",
    "for relation in extracted_relations:\n",
    "    print(relation)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
